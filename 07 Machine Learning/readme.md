## Reference
* [H20](https://www.h2o.ai)
* [Harp](https://dsc-spidal.github.io/harp/)
* [Arxiv](http://arxiv-sanity.com)
* [CS109](http://cs109.github.io/2015/pages/videos.html)
* [PyMC3](https://docs.pymc.io/nb_examples/index.html)
* [Kulbear](https://github.com/Kulbear)
* [LightGBM](https://lightgbm.readthedocs.io/en/latest/)
* [Anaconda](https://www.anaconda.com/anaconda-webinars/)
* [Turi Create](https://github.com/apple/turicreate)
* [Catboost.ai](https://catboost.ai)
* [Featuretools](https://www.featuretools.com)
* [Data School](https://www.dataschool.io)
* [TNSE vs PCA](https://medium.com/@sourajit16.02.93/tsne-t-distributed-stochastic-neighborhood-embedding-state-of-the-art-c2b4b875b7da)
* [ML Explained](http://mlexplained.com)
* [Google AutoML](https://cloud.google.com/automl/)
* [100 page ML Book](http://themlbook.com/wiki/doku.php)
* [Andrew Ng Coursera](https://www.youtube.com/watch?v=qeHZOdmJvFU&index=1&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)
* [Introduction to t-SNE](https://www.datacamp.com/community/tutorials/introduction-t-sne)
* [CapitalOne SageMaker](https://www.capitalone.com/tech/machine-learning/building-machine-learning-models-using-k-means-algorithm-and-amazon-sagemaker)
* [Machine Learning Mastery](https://machinelearningmastery.com)
* [Data Science With No Data](https://towardsdatascience.com/data-science-with-no-data-b3c21acee17c)
* [What Is Manifold Learning?](https://prateekvjoshi.com/2014/06/21/what-is-manifold-learning/)
* [Linear Discriminant Analysis](https://medium.com/@srishtisawla/linear-discriminant-analysis-d38decf48105)
* [The Curse of Dimensionality](https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e)
* [CS 229 ― Machine Learning](https://stanford.edu/~shervine/teaching/cs-229/)
* [Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)
* [Dimensionality Reduction 101](https://towardsdatascience.com/dimensionality-reduction-101-for-dummies-like-me-abcfb2551794)
* [Feature Selection Techniques](https://towardsdatascience.com/feature-selection-techniques-1bfab5fe0784)
* [Principal Component Analysis](https://medium.com/@mallrishabh52/principal-components-analysis-7f6ff559cd83)
* [Machine Learning Cheatsheet](https://ml-cheatsheet.readthedocs.io/en/latest/index.html)
* [Hyperparameters Optimization](https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d)
* [CS 349-02: Machine Learning](http://wellesleynlp.github.io/machinelearning/)
* [Ensemble of Voting Classifiers](https://www.kaggle.com/skooch/ensemble-of-xgboost-lgb-and-randomforest)
* [Hyperparameters Optimization](https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d)
* [Machine Learning In 8 Minutes](https://medium.com/fintechexplained/introduction-to-machine-learning-4b2d7c57613b)
* [Understanding Random Forest](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)
* [A Beginner’s guide to XGBoost](https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7)
* [Open Machine Learning Course](https://mlcourse.ai)
* [Let’s Talk Bayesian Optimization](https://mlconf.com/blog/lets-talk-bayesian-optimization/)
* [Decision Tree in Layman’s Terms](https://towardsdatascience.com/decision-tree-in-laymans-terms-part-1-76e1f1a6b672)
* [5 Powerful Scikit-Learn Examples](https://towardsdatascience.com/5-powerful-scikit-learn-models-e9b12375320d)
* [Introduction to Bayesian Networks](https://towardsdatascience.com/introduction-to-bayesian-networks-81031eeed94e)
* [Chapter 1: Intro to AWS SageMaker](https://towardsdatascience.com/chapter-1-intro-to-aws-sagemaker-a1ecf00ec761)
* [SVM: Feature Selection and Kernels](https://towardsdatascience.com/svm-feature-selection-and-kernels-840781cc1a6c)
* [Get started with Bayesian Inference](https://medium.com/@andreasherman/get-started-with-bayesian-inference-cec9ad4ccd55)
* [A Doomed Marriage of ML and Agile](https://towardsdatascience.com/a-doomed-marriage-of-ml-and-agile-b91b95b37e35)
* [Dimensionality Reduction and UMAP](https://cw.fel.cvut.cz/b181/_media/courses/xp36vpd/vpd_dimensionality_reduction.pdf)
* [Cross Validation: A Beginner’s Guide](https://towardsdatascience.com/cross-validation-a-beginners-guide-5b8ca04962cd)
* [Building Intuition for Random Forests](https://medium.com/x8-the-ai-community/building-intuition-for-random-forests-76d36fa28c5e)
* [Overview of feature selection methods](https://towardsdatascience.com/overview-of-feature-selection-methods-a2d115c7a8f7)
* [Building Blocks of Supervised Learning](https://towardsdatascience.com/building-blocks-of-supervised-learning-cebab32c15ac)
* [A Tour of Machine Learning Algorithms](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)
* [Discriminative vs. Generative Classifiers](https://towardsdatascience.com/a-generative-approach-to-classification-17a0b5876729)
* [What are Eigenvalues and Eigenvectors?](https://medium.com/fintechexplained/what-are-eigenvalues-and-eigenvectors-a-must-know-concept-for-machine-learning-80d0fd330e47)
* [Get the Optimal K in K-Means Clustering](https://medium.com/towards-artificial-intelligence/get-the-optimal-k-in-k-means-clustering-d45b5b8a4315)
* [Feature Scaling with Python’s scikit-learn](https://medium.com/towards-artificial-intelligence/feature-scaling-with-pythons-scikit-learn-10ab42119ae0)
* [7 Things You Should Know about ROC AUC](https://medium.com/hiredscore-engineering/7-things-you-should-know-about-roc-auc-b4389ea2b2e3)
* [Yellowbrick: Machine Learning Visualization](https://www.scikit-yb.org/en/latest/index.html#)
* [The Curse of Dimensionality in classification](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)
* [Machine Learning Model Fairness in Practice](https://medium.com/swlh/machine-learning-model-fairness-in-practice-bdebeaa76ee8)
* [What is Hidden in the Hidden Markov Model?](https://medium.com/acing-ai/what-is-hidden-in-the-hidden-markov-models-eee7bab45ac3)
* [Introduction to Principal Component Analysis](https://towardsdatascience.com/introduction-to-principle-component-analysis-d705d27b88b6)
* [Dimensionality Reduction II: Feature Extraction](https://sebastianraschka.com/pdf/lecture-notes/stat479fs18/14_feat-extract_slides.pdf)
* [How to measure distances in machine learning](https://towardsdatascience.com/how-to-measure-distances-in-machine-learning-13a396aa34ce)
* [Support Vector Machine vs Logistic Regression](https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f)
* [Which machine learning algorithm should I use?](https://blogs.sas.com/content/subconsciousmusings/2017/04/12/machine-learning-algorithm-use/)
* [The Kernel Trick in Support Vector Classification](https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f)
* [A Beginners Guide to Random Forest Regression](https://medium.com/datadriveninvestor/random-forest-regression-9871bc9a25eb)
* [Machine Learning Algorithms for Every Occasion](https://towardsdatascience.com/machine-learning-algorithms-for-every-occasion-90a0ef2e63b6)
* [Clustering metrics better than the elbow-method](https://towardsdatascience.com/clustering-metrics-better-than-the-elbow-method-6926e1f723a6)
* [Unsupervised Learning: Dimensionality Reduction](https://towardsdatascience.com/unsupervised-learning-dimensionality-reduction-ddb4d55e0757)
* [Automate Hyperparameter Tuning for your models](https://towardsdatascience.com/automate-hyperparameter-tuning-for-your-models-71b18f819604)
* [Dealing with the Lack of Data in Machine Learning](https://medium.com/predict/dealing-with-the-lack-of-data-in-machine-learning-725f2abd2b92)
* [A Deep Dive Into Imbalanced Data: Over-Sampling](https://towardsdatascience.com/a-deep-dive-into-imbalanced-data-over-sampling-f1167ed74b5)
* [Python Libraries for Interpretable Machine Learning](https://towardsdatascience.com/python-libraries-for-interpretable-machine-learning-c476a08ed2c7)
* [Detecting Credit Card Fraud Using Machine Learning](https://towardsdatascience.com/detecting-credit-card-fraud-using-machine-learning-a3d83423d3b8)
* [My Favorite Data Science/Machine Learning Resources](https://medium.com/swlh/my-favorite-data-science-machine-learning-resources-6a76bd30b059)
* [An introduction to Amazon Web Services — SageMaker](https://medium.com/towards-artificial-intelligence/aws-sagemaker-32af6c18d7f0)
* [Understanding Decision Trees for Classification (Python)](https://towardsdatascience.com/understanding-decision-trees-for-classification-python-9663d683c952)
* [Spectral Clustering Algorithm Implemented From Scratch](https://towardsdatascience.com/unsupervised-machine-learning-spectral-clustering-algorithm-implemented-from-scratch-in-python-205c87271045)
* [Top 20 APIs You Should Know In AI and Machine Learning](https://towardsdatascience.com/top-20-apis-you-should-know-in-ai-and-machine-learning-8e08515198b3)
* [Why and How to do Cross Validation for Machine Learning](https://towardsdatascience.com/why-and-how-to-do-cross-validation-for-machine-learning-d5bd7e60c189)
* [Balancing Act in Datasets of a Machine Learning Algorithm](https://medium.com/towards-artificial-intelligence/balancing-act-eb75dd05ffa3)
* [How do you take a machine learning model to production?](https://www.quora.com/How-do-you-take-a-machine-learning-model-to-production/answer/Håkon-Hapnes-Strand)
* [6 amateur mistakes I’ve made working with train-test splits](https://towardsdatascience.com/6-amateur-mistakes-ive-made-working-with-train-test-splits-916fabb421bb)
* [From Machine Learning to Reinforcement Learning Mastery](https://medium.com/better-programming/from-machine-learning-to-reinforcement-learning-mastery-47f33d9f6b41)
* [A step by step explanation of Principal Component Analysis](https://towardsdatascience.com/a-step-by-step-explanation-of-principal-component-analysis-b836fb9c97e2)
* [How to code Gaussian Mixture Models from scratch in Python](https://towardsdatascience.com/how-to-code-gaussian-mixture-models-from-scratch-in-python-9e7975df5252)
* [The 5 most useful Techniques to Handle Imbalanced datasets](https://towardsdatascience.com/the-5-most-useful-techniques-to-handle-imbalanced-datasets-6cdba096d55a)
* [Machine Learning Cheat Sheet — Data Processing Techniques](https://medium.com/swlh/machine-learning-cheat-sheet-data-processing-71faf3b26db8)
* [Top Data Science and Machine Learning Methods Used in 2019](https://www.kdnuggets.com/2019/04/top-data-science-machine-learning-methods-2018-2019.html)
* [The Ultimate Guide to 12 Dimensionality Reduction Techniques](https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/)
* [Let’s Talk About Machine Learning Ensemble Learning In Python](https://medium.com/fintechexplained/lets-talk-about-machine-learning-ensemble-learning-in-python-382747e5fba8)
* [Principal Component Analysis and SVM in a Pipeline with Python](https://towardsdatascience.com/visualizing-support-vector-machine-decision-boundary-69e7591dacea)
* [Feature Transformation for Machine Learning, a Beginners Guide](https://medium.com/vickdata/four-feature-types-and-how-to-transform-them-for-machine-learning-8693e1c24e80)
* [Supervised Learning: Basics of Classification and Main Algorithms](https://towardsdatascience.com/supervised-learning-basics-of-classification-and-main-algorithms-c16b06806cd3)
* [Performance Comparison of Dimension Reduction Implementations](https://umap-learn.readthedocs.io/en/latest/benchmarking.html)
* [Support Vector Machine (SVM): A Simple Visual Explanation — Part 1](https://medium.com/towards-artificial-intelligence/support-vector-machine-svm-a-visual-simple-explanation-part-1-a7efa96444f2)
* [The 5 Feature Selection Algorithms every Data Scientist should know](https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2)
* [Visualising high-dimensional datasets using PCA and t-SNE in Python](https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b)
* [Feature Preprocessing for Numerical Data — The Most Important Step](https://medium.com/analytics-vidhya/feature-preprocessing-for-numerical-data-the-most-important-step-e9ed76151298)
* [A Detailed Guide to 7 Loss Functions for Machine Learning Algorithms](https://medium.com/analytics-vidhya/a-detailed-guide-to-7-loss-functions-for-machine-learning-algorithms-26e11b6e700b)
* [An easy introduction to unsupervised learning with 4 basic techniques](https://towardsdatascience.com/an-easy-introduction-to-unsupervised-learning-with-4-basic-techniques-da7fbf0c3adf)
* [What is Principal Component Analysis (PCA) and when should I use it?](https://medium.com/@josecacho/what-is-principal-component-analysis-pca-and-when-should-i-use-it-6305e880d69)
* [Support Vector Machine — Introduction to Machine Learning Algorithms](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)
* [A Complete Machine Learning Project Walk-Through in Python: Part One](https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420)
* [Essential Cheat Sheets for Machine Learning and Deep Learning Engineers](https://startupsventurecapital.com/essential-cheat-sheets-for-machine-learning-and-deep-learning-researchers-efb6a8ebd2e5)
* [A Machine Learning Approach — Building a Hotel Recommendation Engine](https://towardsdatascience.com/a-machine-learning-approach-building-a-hotel-recommendation-engine-6812bfd53f50)
* [An easy guide to choose the right Machine Learning algorithm for your task](https://medium.com/dataseries/an-easy-guide-to-choose-the-right-machine-learning-algorithm-for-your-task-b0f6d77aab75)
* [Built-in Machine Learning Algorithms with Amazon SageMaker - a Deep Dive](https://www.youtube.com/watch?v=yGc0qePSYig&feature=youtu.be&list=PLhr1KZpdzukcOr_6j_zmSrvYnLUtgqsZz)
* [Boosting and Bagging: How To Develop A Robust Machine Learning Algorithm](https://medium.com/better-programming/how-to-develop-a-robust-algorithm-c38e08f32201)
* [Best Practices for Hyperparameter Tuning with MLflowJoseph Bradley Databricks](https://youtu.be/eEVDIOiYuMk)
* [Understanding the 3 most common loss functions for Machine Learning Regression](https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3)
* [How To Out-Compete On a Data Science Competition — Insights, Techniques And Tactics](https://towardsdatascience.com/how-to-out-compete-on-a-data-science-competition-insights-techniques-and-tactics-95a0545041d5)
* [AWS Builders' Day | Machine Learning: From Notebook to Production with Amazon Sagemaker](https://youtu.be/sLktFah2Xi4)
* [AWS re:Invent 2018: CI/CD for Your Machine Learning Pipeline with Amazon SageMaker (DVC303)](https://youtu.be/6EYEoAqihPg)
* [Do most machine learning algorithms run in batch, or do they run every time they get a new bit of data?](https://www.quora.com/Do-most-machine-learning-algorithms-run-in-batch-or-do-they-run-every-time-they-get-a-new-bit-of-data/answer/Håkon-Hapnes-Strand)
* [How to Use a Machine Learning Checklist to Get Accurate Predictions, Reliably (even if you are a beginner)](https://machinelearningmastery.com/machine-learning-checklist/)

## Whiteboard ML Workflow
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/WhiteboardMLWorkFlow.png)

## AWS Machine Learning Stack 2020
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/AWSMLStack2020.png)

## Data Science is 70% classification problems
Three of the most popular classifiers within Machine Learning (which use 3 different mathematical approaches to classify data) are:
1) Naive Bayes, which uses a statistical (Bayesian) approach
2) Logistic Regression, which uses a functional approach
3) Support Vector Machines, which uses a geometrical approach
4) XGBoost
5) [AWS Classification](https://youtu.be/yGc0qePSYig?list=PLhr1KZpdzukcOr_6j_zmSrvYnLUtgqsZz&t=281)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/AWSAlogorithms.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/AWSBuiltInAlgorithms.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/MicrosoftAzureMachineLearningAlgorithmCheatSheet.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/azure-machine-learning-algorithm-cheat-sheet-nov2019.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/MachineLearningAlgorithmsCheatSheet.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/TheMachineLearningProcess.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/HPOApproaches.png)

## Comparing Machine Learning Models
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/ComparingMachineLearningModels.png)

![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/AWSMLStack.png)

## Dimensionality Reduction
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/DimensionalityReductionTechniques.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/OverallAccuraciesForDifferentlyReducedDataSets.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/CurseOfDimensionality.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/LDAPCA.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/MatrixFactorization.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/NeighbourGraphs.png)

## Sagemaker Lifecycle
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/SagemakerLifecycle.png)

## Building Blocks of Supervised Learning
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/BuildingBlocksSupervisedLearning.png)

## Machine Learning Decision Tree
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/SupervisedvsUnsupervised.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/Classification-Machine-Learning-Algorithm.png)

## Kaggle Winners
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/KaggleWinners.png)

## Classification & Regression Loss Functions
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/ClassificationAndRegressionLossFunctions.png)

## Common Evaluation Metrics
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/CommonEvaluationMetrics.png)

## Common Machine Learning Alogrithms
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/CommonMachineLearningAlgorithms.jpg)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/MachineLearningAlgorithms_01.jpg)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/MachineLearningAlgorithms_02.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/MachineLearningAlgorithms_03.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/SVMApplication.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/SVM.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/SVMHyperPlane.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/SVMKernelTrick.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/EvolutionOfXGBoostAlgorithmFromDecisionTrees.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/Bagging.png)

## Methods to Deploy Algorithms to Production
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/DeployAlgorithmsToProduction.png)

## 100 Page Machine Learning Book
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/100pageMLBook.png)

## Model Tuning
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/L1L2Regularization_02.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/L1L2Regularization_01.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/AUCScores.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/ImbalancedClasses.png)

## Precision vs Recall
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/ComprehensiveConfusionMatrix.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/Confusion.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/Accuracy.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/PrecisionRecallF1.png)

## Overfitting vs Underfitting
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/Overfitting.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/DataSetSize.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/OptimalModel.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/ValidationCurve_01.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/ValidationCurve_02.png)

## Bias vs Variance
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/BiasVariance.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/ValidationCurve_03.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/ValidationCurve_04.png)

## Cross Validation
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/TrainingValidationTesting.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/ShuffleDataToBalanceData.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/HoldoutValidation.png)

## Hyperparameter Tuning & Model Training
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/HyperParameterTuning_02.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/HyperParameterTuning_01.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/OnlineOfflineEvaluation.png)

## Machine Learning Checklist
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/MachineLearningChecklist.png)

## Dealing with a Lack of Data
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/LackOfData_01.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/LackOfData_02.png)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/LackOfData_03.png)

## Flexbility Interpretability Trade-off
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/FlexibilityInterpretabilityTradeOff.png)

## Weighted Voter (Ensemble Method)
![](https://github.com/geoffreylink/Projects/blob/master/07%20Machine%20Learning/images/WeightedVoter.png)
